\subsection{Results} \label{Sec:results}

%\input{covg-table} 


%\headbf{Function coverage maximization (RQ1)} \tabref{efficiency-abs-mut-table} presents the statement coverage achieved by using our function coverage maximization technique and the random strategy. Our results show on average 9\% improvement in statement coverage, across all the applications. We observed that our technique achieves the highest improvement when there are dynamically generated clickables in the application.
%
%For example, for Tunnel (ID 2) and Fractal Viewer (ID 9), we observed no improvement in the statement coverage as these two applications have no dynamically generated or bound to event-listeners clickables. Instead, their few clickables are all placed in the HTML code of the application with a fixed event-handler per clikcable. Thus,  our approach achieves the same coverage as the random strategy for such applications.
%
%For SameGame (ID 1) the number of executed functions remains the same, for both approaches. However, in the limited five minute period of time, as a result of dynamically detecting valid clickable elements, \tool is able to examine different paths of the applications, while the random exploration technique fails to do so as it blindly clicks on any candidate element on the DOM tree.
%
%Finally, GhostBusters (ID 3) benefits the most from our dynamic exploration technique. This application contains a large number of dynamically created clickable DOM elements that appear and then disappear within a few seconds. We observed two anonymous functions, attached to clickable elements, that are missed by the random exploration technique in this application. Our technique is not only able to quickly spot such on the fly generated clickables, but it is also able to click on the ones that result in the execution of uncovered functions. 
%These two functions are attached to clickable elements that are not executed using random strategy. 

\input{efficiency-abs-mut-table}

\begin{figure}[!t]
  \centering
  \includegraphics[width=1\hsize]{r-scripts/barplot_coverage}
  \vspace{-0.1in} 
  \mycaption{Statement coverage achieved.} 
  \vspace{-0.25in} 
  \label{Fig:coverage-graph}
\end{figure}


\headbf{Test Case Generation (RQ1)} 
\figref{coverage-graph} depicts the statement coverage achieved by \tool for each application. The results show that the test cases generated by \tool achieve a coverage of 68.4\% on average, ranging from 41\% (ID 12) up to 99\% (ID 1).
%\karthik{Do we need the text below ? We are better than random.}
We investigated why \tool has low coverage for some of the applications. For instance, we observed that in JointLondon (ID 7), the application contains \javascript functions that are browser/device specific, i.e., they are exclusively executed in Internet Explorer, or iDevices. As a result, we are unable to cover them using \tool. 
We also noticed that some applications required more time to achieve higher statement coverage (e.g., in NarrowDesign ID 8), or they have a large DOM state space (e.g., BunnyHunt ID 5) and hence \tool is only able to cover a portion of these applications in the limited time it had available.

\tabref{efficiency-abs-mut-table} columns under ``St. Coverage'' present \javascript statement coverage achieved by  our function coverage maximization algorithm versus a random strategy. The results show a 9\% improvement on average, for our algorithm, across all the applications. We observed that our technique achieves the highest improvement when there are many dynamically generated clickable DOM elements in the application, for example, GhostBusters (ID 3). 
%For instance, GhostBusters (ID 3) has 27\% more coverage since it contains a large number of dynamically created clickable DOM elements that appear and then disappear within a few seconds, which our technique can handle, but the random one may not. 
%We observed two anonymous functions, attached to clickable elements, that are missed by the random exploration technique in this application. Our technique is not only able to quickly spot such on the fly generated clickables, but it is also able to click on the ones that result in the execution of uncovered functions.

The columns under ``State Abstract'' in \tabref{efficiency-abs-mut-table} present the number of function states
before and after applying our function state abstraction algorithm.   
The results show that the abstraction strategy reduces function states by 85.5\% on average. NarrowDesign (ID 7) and FractalViewer (ID 9) benefit the most by a 97\% state reduction rate. 
%\karthik{What about ID 9 ?}
Note that despite this huge reduction, our state abstraction does not adversely influence the coverage as we include at least one function state from each of the covered branch sets as described in \secref{testCaseGen}.

The last two columns of \tabref{efficiency-abs-mut-table}, under ``Oracles'', present the number of assertions obtained by capturing the whole application's state,  without any mutations, and with our mutation-based oracle generation algorithm respectively. The results show that the number of assertions is decreased by 86.5\% on average due to 
our algorithm. 
We observe the most significant reduction of assertions for JointLondon (ID 7) from more than 198000 to 342. 
%\ali{by how much? what is the reduction rate?} 


%\ali{removing the insiginificant results} For example, for Tunnel (ID 2) and Fractal Viewer (ID 9), we observed no improvement in the statement coverage as these two applications have no dynamically generated or bound to event-listeners clickables. Instead, their few clickables are all placed in the HTML code of the application with a fixed event-handler per clikcable. Thus,  our approach achieves the same coverage as the random strategy for such applications.
%For SameGame (ID 1) the number of executed functions remains the same, for both approaches. However, in the limited five minute period of time, as a result of dynamically detecting valid clickable elements, \tool is able to examine different paths of the applications, while the random exploration technique fails to do so as it blindly clicks on any candidate element on the DOM tree.



%These two functions are attached to clickable elements that are not executed using random strategy.
%\figref{coverage-graph} presents our results for the statement coverage achieved by the test suite generated by \tool and \artemis. %The coverage is shown separately for the function-level unit tests and DOM event-based tests.     
%\tabref{efficiency-abs-mut-table} presents the number of function states
%before and after applying our function state abstraction mechanism.    
%The results show that the abstraction strategy is able to reduce the number of function states up to 97\%. NarrowDesign (id 7) with the largest
%number of function states benefits the most from our abstraction technique by 97\% state reduction. Note that we observed that our state abstraction does not affect the coverage. The reason is we select at least one function state from each of the covered branch sets as described in \secref{testCaseGen}.
%The last two columns of \tabref{efficiency-abs-mut-table} presents the number of all possible assertions obtained by capturing the whole application's state without performing any mutation, as well as the number of assertions after applying mutation. The results show that the number of assertions are drastically reduced by selectively pick only useful assertions during the mutation process.
% \tabref{testability-table} presents the testability of \javascript functions in the experimental objects.
% The table shows the total number of functions, number of functions that are testable, and the percentage of testable functions. 
% It also shows the number and the percentage of testable functions that are tested by the function-level tests generated by \tool (See Definitions \ref{testabilityFormula}-\ref{pythiaTestabilityFormula} in Section \ref{test-gen-setup}). 
% 
%As shown in \tabref{testability-table}, our generated function-level unit tests can, on average, examine 77\% of the testable functions. 
%This demonstrates the efficacy of our function covering technique in covering a considerable number of functions. 
%While \tool is able to examine up to 100\% of the testable functions for  SameGame, it can only cover 48\% of the testable functions for Fractal Viewer. 
%The low numbers for Fractal Viewer are mainly due to (1) the presence of random generator functions, and (2) the lack of support  for object input parameters with cyclic references in the current implementation of \tool, which we discuss in \secref{discussion}.

\input{faultDetection-table}
\headbf{Fault finding capability (RQ2)} \tabref{faultDetection-table} presents the results on the fault finding capabilities of \tool.
The table shows the total number of injected faults, the number of false negatives, false positives, true positives, and the precision and recall of \tool. 

\tool achieves 100\% precision, meaning that all the detected faults reported by \tool are real faults. {\em In other words, there are no false-positives.}
This is because the assertions generated by \tool are all stable \ie they do not change from one run to another. % as we do not observe any false positives. 
However, the recall of \tool is 70\% on average, and ranges from 48 to 100\%. This is due to false negatives, \ie missed faults by \tool, 
which occur when the injected fault falls is either in the uncovered region of the application, or is not properly captured by the generated oracles.  

%\begin{figure}[!t]
%  \centering
%  \includegraphics[width=0.9\hsize]{r-scripts/recall}
%  \mycaption{Recall of \tool and \artemis in detecting faults.}
%  \vspace{-0.1in} 
%  \label{Fig:recall-graph}
%\end{figure}

The table also shows that on average 32\% percent of the injected faults (ranges from 15--73\%) are detected by function-level test cases, but not by our DOM event-based test cases. This shows that a considerable number of faults do not propagate to observable DOM states, and thus cannot be captured by DOM-level event-based tests. 
For example in the SimpleCart application (ID 10), if we mutate the mathematical operation that is responsible for computing the total amount of purchased items, the resulting error is not captured by event-based tests as the fault involves internal computations only. However, the fault is detected by a function-level test that directly checks the returned value of the function.
This points to the importance of incorporating function-level tests in addition to event-based tests for \javascript web applications. We also observed that even when an event-based test case detects a \javascript fault, localizing the error to the corresponding \javascript code can be quite challenging. However, function-level tests pinpoint the  corresponding function when an assertion fails, making it easier to localize the fault. 
%For example, in SameGame (ID 1), the function \code{compactDown} is responsible for moving cells up on the board after clicking on a given cell. If the index of the global variable \code{board} array is mutated in \code{compactDown}, the event-based tests shows an incorrect \code{background} value of the \code{css} property on the corresponding DOM elements. However, tracing the fault back to the responsible function is difficult. Using function-level tests, we can easily spot the assertion that is failed after calling \code{compactDown} function (e.g., \code{equal(board[3][4], 1)} failed).    

\headbf{Comparison (RQ3)}
\figref{coverage-graph} shows the code coverage achieved by  both \tool and \artemis on the experimental objects running for the same amount of time, \ie 10 minutes.
%Note that while \artemis only generates DOM event tests, \tool generates both unit tests and DOM event tests. 
%We compare the statement coverage achieved by \tool with that achieved by \artemis. 
The test cases generated by \tool achieve 68.4\% coverage on average (ranging from 41--99\%), while those generated by \artemis achieve only 44.8\% coverage on average (ranging from 0--92\%).
Overall, the test cases generated by \tool achieve 53\% more coverage than \artemis, which points to the effectiveness of \tool in generating high coverage test cases. 
Further, as can be seen in the bar plot of \figref{coverage-graph}, for all the applications, the test cases generated by \tool achieve higher coverage than those generated by \artemis. 
This increase was more than 226\% in the case of Bunnyhunt (ID 5). %\ali{Is this true? Isn't the difference in ID 7 and ID 8 more?}
For two of the applications, NarrowDesign (ID 7) and JointLondon (id 8), \artemis was not able to complete the testing task within the allocated time of ten minutes.
Thus we let \artemis run for an additional 10 minutes for these applications (\ie 20 minutes in total). Even then, neither application completes under \artemis. 
%By reducing the iteration option, \artemis produces an output after 10 minutes, however, with considerably lower coverage rates of 30\% and 35\% for NarrowDesign (ID 7), and JointLondon (ID  8), respectively. %I calculated the results based on zero coverage for these two apps, not sure if mentioning 30 and 35% here is correct or not!   

\tabref{faultDetection-table} shows the precision and recall achieved by \tool and \artemis.
With respect to fault finding capability, unlike \artemis that detects only generic faults such as runtime exceptions and W3C HTML validation errors, \tool is able to accurately distinguish faults at the code-level and DOM-level through the test oracles it generates. Both tools achieve 100\% precision, however, \tool achieves five-fold higher recall (70\% on average) compared with \artemis (12.8\% recall on average). %Thus, \tool achieves more than five-fold increase in recall over \artemis.   
